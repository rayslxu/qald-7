"use strict";
// -*- mode: typescript; indent-tabs-mode: nil; js-basic-offset: 4 -*-
//
// This file is part of Genie
//
// Copyright 2020 The Board of Trustees of the Leland Stanford Junior University
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//    http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//
// Author: Giovanni Campagna <gcampagn@cs.stanford.edu>
var __createBinding = (this && this.__createBinding) || (Object.create ? (function(o, m, k, k2) {
    if (k2 === undefined) k2 = k;
    var desc = Object.getOwnPropertyDescriptor(m, k);
    if (!desc || ("get" in desc ? !m.__esModule : desc.writable || desc.configurable)) {
      desc = { enumerable: true, get: function() { return m[k]; } };
    }
    Object.defineProperty(o, k2, desc);
}) : (function(o, m, k, k2) {
    if (k2 === undefined) k2 = k;
    o[k2] = m[k];
}));
var __setModuleDefault = (this && this.__setModuleDefault) || (Object.create ? (function(o, v) {
    Object.defineProperty(o, "default", { enumerable: true, value: v });
}) : function(o, v) {
    o["default"] = v;
});
var __importStar = (this && this.__importStar) || function (mod) {
    if (mod && mod.__esModule) return mod;
    var result = {};
    if (mod != null) for (var k in mod) if (k !== "default" && Object.prototype.hasOwnProperty.call(mod, k)) __createBinding(result, mod, k);
    __setModuleDefault(result, mod);
    return result;
};
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
const events = __importStar(require("events"));
const speech_recognizer_1 = __importDefault(require("./speech_recognizer"));
const speech_synthesizer_1 = __importDefault(require("./speech_synthesizer"));
const protocol_1 = require("../dialogue-agent/protocol");
const custom_error_1 = __importDefault(require("../utils/custom_error"));
class LocalAudioPlayer {
    constructor(handler, platform, conversation) {
        this.conversationId = conversation.id;
        this._handler = handler;
        this._platform = platform;
        this._cap = platform.getCapability('audio-player');
        this._player = null;
    }
    async checkCustomPlayer(spec) {
        return spec.type === 'url';
    }
    async prepare(spec) {
        if (spec && spec.type !== 'url')
            throw new Error(`unsupported`);
        // wait until the agent finishes speaking to start playing audio
        await this._handler.waitFinishSpeaking();
    }
    async resume() {
        throw new custom_error_1.default(`unsupported`, `Resuming is not supported`);
    }
    async stop() {
        if (this._player)
            await this._player.stop();
        this._player = null;
    }
    async pause() {
        // we don't have a way to pause, so we just stop
        if (this._player)
            await this._player.stop();
        this._player = null;
    }
    async playURLs(urls) {
        await this.stop();
        this._player = await this._cap.play(urls);
    }
    async setVolume(volume) {
        throw new Error('Method not implemented.');
    }
    async adjustVolume(delta) {
        throw new Error('Method not implemented.');
    }
    async setMute(mute) {
        throw new Error('Method not implemented.');
    }
    async setVoiceInput(input) {
        const prefs = this._platform.getSharedPreferences();
        prefs.set('enable-voice-input', input);
        this._handler.setVoiceInput(input);
    }
    async setVoiceOutput(input) {
        const prefs = this._platform.getSharedPreferences();
        prefs.set('enable-voice-output', input);
        this._handler.setVoiceOutput(input);
    }
}
class SpeechHandler extends events.EventEmitter {
    constructor(conversation, platform, options = {
        nlUrl: 'https://almond-nl.stanford.edu'
    }) {
        var _a, _b;
        super();
        this._stream = null;
        this._platform = platform;
        this._prefs = platform.getSharedPreferences();
        this._conversation = conversation;
        this._audioController = conversation.engine.audio;
        this._pulse = platform.getCapability('sound');
        this._wakeWordDetector = platform.getCapability('wakeword-detector');
        this._voiceDetector = platform.getCapability('voice-detector');
        this._systemLock = platform.getCapability('system-lock');
        if (this._wakeWordDetector) {
            this._wakeWordDetector.on('wakeword', (wakeword, buffer) => {
                if (this._systemLock && this._systemLock.isActive) {
                    console.log('Ignored wakeword ' + wakeword + ' because the system is locked');
                    return;
                }
                console.log('Wakeword ' + wakeword + ' detected');
                this._onDetected(buffer);
            });
        }
        this._recognizer = new speech_recognizer_1.default({
            locale: this._platform.locale,
            nlUrl: options.nlUrl,
            vad: this._voiceDetector
        });
        this._recognizer.on('error', (e) => {
            this.emit('error', e);
        });
        this._tts = new speech_synthesizer_1.default(platform, options.nlUrl);
        this._currentRequest = null;
        this._queuedAudio = [];
        this._started = true;
        this._enableVoiceInput = (_a = this._prefs.get('enable-voice-input')) !== null && _a !== void 0 ? _a : true;
        this._enableVoiceOutput = (_b = this._prefs.get('enable-voice-output')) !== null && _b !== void 0 ? _b : true;
        this._prefs.on('changed', (key) => {
            var _a, _b;
            if (key === 'enable-voice-input')
                this.setVoiceInput((_a = this._prefs.get('enable-voice-input')) !== null && _a !== void 0 ? _a : true);
            else if (key === 'enable-voice-output')
                this.setVoiceOutput((_b = this._prefs.get('enable-voice-output')) !== null && _b !== void 0 ? _b : true);
        });
        this._player = null;
        if (this._platform.hasCapability('audio-player'))
            this._player = new LocalAudioPlayer(this, platform, conversation);
    }
    setVoiceInput(enable) {
        if (enable === this._enableVoiceInput)
            return;
        this._enableVoiceInput = enable;
        if (this._started && enable)
            this._startVoiceInput();
        else
            this._stopVoiceInput();
    }
    setVoiceOutput(enable) {
        if (enable === this._enableVoiceOutput)
            return;
        this._enableVoiceOutput = enable;
        if (!enable)
            this._tts.clearQueue();
    }
    // called from conversation
    async setHypothesis() {
        // ignore, this is called from the conversation when it broadcasts the hypothesis
        // to all listeners
    }
    async addDevice() {
        // ignore
    }
    waitFinishSpeaking() {
        return new Promise((resolve, reject) => {
            if (!this._tts.speaking) {
                resolve();
                return;
            }
            this._tts.once('done', () => resolve());
        });
    }
    async setExpected(expect) {
        // flush any request to play audio
        if (this._queuedAudio.length) {
            const toPlay = this._queuedAudio;
            this._queuedAudio = [];
            if (!this._player)
                return;
            const player = this._player;
            await this._audioController.requestSystemAudio({
                async stop() {
                    await player.stop();
                }
            }, this._conversation.id);
            await player.playURLs(toPlay);
        }
    }
    async addMessage(message) {
        switch (message.type) {
            case protocol_1.MessageType.COMMAND:
                await this._tts.clearQueue();
                break;
            case protocol_1.MessageType.TEXT:
                if (!this._enableVoiceOutput)
                    break;
                await this._tts.say(message.text);
                break;
            case protocol_1.MessageType.SOUND_EFFECT: {
                const soundEffects = this._platform.getCapability('sound-effects');
                if (soundEffects) {
                    if (message.exclusive) {
                        // in exclusive mode, we queue the sound effect as if it was
                        // a regular audio URL
                        // this means we'll stop other audio and we will synchronize
                        // with audio messages
                        const url = soundEffects.getURL(message.name);
                        if (url)
                            this._queuedAudio.push(url);
                        else
                            console.log(`Ignored unknown sound effect ${message.name}`);
                    }
                    else {
                        if (!this._enableVoiceOutput)
                            break;
                        this.waitFinishSpeaking().then(() => {
                            return soundEffects.play(message.name);
                        }).catch((e) => {
                            console.error(`Failed to play sound effect: ${e.message}`);
                        });
                    }
                }
                else {
                    console.log(`Ignored sound effect ${message.name}: not supported on this platform`);
                }
                break;
            }
            case protocol_1.MessageType.AUDIO:
                this._queuedAudio.push(message.url);
                break;
            // ignore all other message types
        }
    }
    /**
     * Programmatically trigger a wakeword.
     *
     * This can be used to emulate a wakeword with a push button.
     */
    wakeword() {
        this._onDetected(Buffer.from([]));
    }
    _onDetected(buffer, mustHaveWakeword = true) {
        // if we already have a request active, ignore the wakeword, we're
        // already streaming the sound to the server
        if (this._currentRequest)
            return;
        this.emit('wakeword');
        this._currentRequest = this._recognizer.request(this._stream, buffer);
        this._currentRequest.on('hypothesis', (hypothesis) => {
            this._conversation.setHypothesis(hypothesis);
        });
        this._currentRequest.on('done', (status, utterance) => {
            this._currentRequest = null;
            if (status === 'Success') {
                console.log('Recognized as "' + utterance + '"');
                if (mustHaveWakeword) {
                    const wakeWordMatch = /^(computer)[,.!]?/i.exec(utterance);
                    if (!wakeWordMatch) {
                        console.log('Ignored because wake-word is missing');
                        this.emit('no-match');
                        return;
                    }
                    // remove the prefix from the utterance so we don't confuse
                    // the model
                    utterance = utterance.substring(wakeWordMatch[0].length).trim();
                }
                // if there is nothing left, start listening again in case
                // the user paused in-between the wakeword and the command
                // in that case, we will not check for the wakeword and remove it
                //if (!utterance) {
                //    this._onDetected(Buffer.from([]), false);
                //    return;
                //}
                this.emit('match');
                this._conversation.setHypothesis('');
                this._conversation.handleCommand(utterance);
            }
            else if (status === 'NoMatch' || status === 'InitialSilenceTimeout') {
                this.emit('no-match');
            }
            else {
                console.log('Recognition error: ' + status);
            }
        });
        this._currentRequest.on('error', (error) => {
            this._currentRequest = null;
            this._onError(error);
        });
    }
    _onError(error) {
        console.log('Error in speech recognition: ' + error.message);
        this._tts.say("Sorry, I had an error understanding your speech: " + error.message);
    }
    start() {
        this._conversation.addOutput(this, false);
        if (this._player)
            this._audioController.addPlayer(this._player);
        this._started = true;
        if (this._enableVoiceInput)
            this._startVoiceInput();
    }
    _startVoiceInput() {
        this._stream = this._pulse.createRecordStream({
            format: 'S16LE',
            rate: 16000,
            channels: 1,
            stream: 'genie-voice-output',
            properties: {
                'media.role': 'voice-assistant',
                'filter.want': 'echo-cancel',
            }
        });
        this._stream.on('state', (state) => {
            console.log('Record stream is now ' + state);
            if (state === 'ready')
                this.emit('ready');
        });
        if (this._voiceDetector)
            this._voiceDetector.setup(16000, 3);
        if (this._wakeWordDetector)
            this._stream.pipe(this._wakeWordDetector);
    }
    destroy() {
        if (this._player) {
            this._player.stop().catch((e) => {
                console.error(`Failed to stop playback: ${e.message}`);
            });
            this._audioController.removePlayer(this._player);
        }
        this._conversation.removeOutput(this);
        this._started = false;
        this._stopVoiceInput();
        this._tts.clearQueue();
    }
    _stopVoiceInput() {
        if (!this._stream)
            return;
        this._stream.unpipe();
        this._stream.end();
        this._stream = null;
        this._recognizer.close();
    }
}
exports.default = SpeechHandler;
//# sourceMappingURL=speech_handler.js.map